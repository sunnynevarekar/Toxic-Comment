{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents a baseline model for Kaggle's recently completed Toxic Comment Classification Challenge.\n",
    "I have used pretrained GloVe word vctors. I have tried to mimic the idea of attention mechanism present in the paper https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf. The basic idea is, not all words contribute equally to the representation of the sentence meaning. Hence, the authors of this papers introduced attention mechanism to extract such words that are important to the meaning of the sentence and aggregate the representation of those informative words to form a sentence vector. The LB score for this model was 0.9840. I trained model only for 3 epochs. No text cleaning or special preprocessing was done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, Input, Bidirectional, LSTM, GRU, Embedding, Reshape, Multiply, Activation\n",
    "from keras.layers import Conv1D, Dropout, GlobalAveragePooling1D, GlobalMaxPooling1D, SpatialDropout1D\n",
    "from keras.layers import Lambda, concatenate, TimeDistributed, BatchNormalization\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model,Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = 'glove.840B.300d.txt'\n",
    "TRAIN_DATA_FILE = 'train.csv'\n",
    "TEST_DATA_FILE = 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300          #word vector size\n",
    "max_features = 100000     #max words to work with\n",
    "max_len = 150             #max number of words in a comment to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of comments in training data: 159571\n",
      "Total number of comments in test data: 153164\n"
     ]
    }
   ],
   "source": [
    "#Load data\n",
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "print(\"Total number of comments in training data: {}\".format(train.shape[0]))\n",
    "print(\"Total number of comments in test data: {}\".format(test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00025465d4725e87</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00031b1e95af7921</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00037261f536c51d</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00040093b2687caa</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "5  00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...      0   \n",
       "6  0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1   \n",
       "7  00031b1e95af7921  Your vandalism to the Matt Shirvington article...      0   \n",
       "8  00037261f536c51d  Sorry if the word 'nonsense' was offensive to ...      0   \n",
       "9  00040093b2687caa  alignment on this subject and which are contra...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  \n",
       "5             0        0       0       0              0  \n",
       "6             1        1       0       1              0  \n",
       "7             0        0       0       0              0  \n",
       "8             0        0       0       0              0  \n",
       "9             0        0       0       0              0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's see few rows of our train data\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate comments and labels as our features and classes\n",
    "classes = ['toxic', 'severe_toxic', 'obscene', 'threat',\n",
    "       'insult', 'identity_hate']\n",
    "train_y = train[classes].values\n",
    "train_x = train['comment_text'].str.lower()\n",
    "test_x = test['comment_text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(id               False\n",
       " comment_text     False\n",
       " toxic            False\n",
       " severe_toxic     False\n",
       " obscene          False\n",
       " threat           False\n",
       " insult           False\n",
       " identity_hate    False\n",
       " dtype: bool, id              False\n",
       " comment_text    False\n",
       " dtype: bool)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Do we have any missing values in our train and test data\n",
    "train.isnull().any(), test.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare GloVe embedding\n",
    "embeddings_index = {}\n",
    "\n",
    "#Read the GloVe word vectors\n",
    "with open(EMBEDDING_FILE, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize text\n",
    "tokenizer = Tokenizer(num_words=max_features, lower=True)\n",
    "tokenizer.fit_on_texts(list(train_x)+list(test_x))\n",
    "train_sequence = tokenizer.texts_to_sequences(train_x)\n",
    "test_sequence = tokenizer.texts_to_sequences(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique tokens found: 394787\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of unique tokens found: {}\".format(len(tokenizer.word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([26691., 22100., 16260., 12907., 10179.,  7728.,  6106.,  4863.,\n",
       "         3922.,  3307.,  2790.,  2397.,  1934.,  1738.,  1464.,  1445.,\n",
       "         1291.,  1092.,   833.,   805.,   776.,   606.,   574.,   521.,\n",
       "          477.,   417.,   391.,   409.,   269.,   296.,   272.,   219.,\n",
       "          186.,   160.,   172.,   152.,   154.,   138.,   137.]),\n",
       " array([ 10,  20,  30,  40,  50,  60,  70,  80,  90, 100, 110, 120, 130,\n",
       "        140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260,\n",
       "        270, 280, 290, 300, 310, 320, 330, 340, 350, 360, 370, 380, 390,\n",
       "        400]),\n",
       " <a list of 39 Patch objects>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndMJUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f68e27215f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's see the distribution of lengths of training sequences\n",
    "train_sequence_length = [len(sequence) for sequence in train_sequence]\n",
    "plt.hist(train_sequence_length, bins=np.arange(10, 410, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 150)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#From distribution above we can see that most of the sequences have length less than 100, \n",
    "#for safer side we will keep max_len as 150\n",
    "x_t = pad_sequences(train_sequence, max_len)\n",
    "x_te = pad_sequences(test_sequence, max_len)\n",
    "x_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words_count = 0\n",
    "uncommon_words =[]\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        break\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        common_words_count +=1\n",
    "    else:\n",
    "        uncommon_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common words in Glove and our vocabulary : 73366\n",
      "Number of uncommon words in Glove and our vocabulary : 26633\n",
      "Shape of Embedding matrix: (100000, 300)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of common words in Glove and our vocabulary : {}\".format(common_words_count))\n",
    "print(\"Number of uncommon words in Glove and our vocabulary : {}\".format(len(uncommon_words)))\n",
    "print(\"Shape of Embedding matrix: {}\".format(embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the training set in train and validation set\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_t, train_y, test_size=0.1, random_state=233) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"weights_base.best.h5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=3)\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "            \n",
    "ra_val = RocAucEvaluation(validation_data=(x_val, y_val), interval=1) \n",
    "callback_list = [ra_val, checkpoint, earlyStopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 150, 300)     30000000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 150, 300)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 150, 256)     439296      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 150, 128)     32768       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 150, 1)       129         time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 150)          0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 150)          0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 150, 1)       0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 150, 256)     0           bidirectional_1[0][0]            \n",
      "                                                                 reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 256)          0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           16448       lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 6)            390         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 30,489,031\n",
      "Trainable params: 30,489,031\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(max_len,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = SpatialDropout1D(0.35)(x)\n",
    "h = Bidirectional(LSTM(128, return_sequences=True, dropout=0.15, recurrent_dropout=0.15))(x)\n",
    "u = TimeDistributed(Dense(128, activation='relu', use_bias=False))(h)\n",
    "alpha = TimeDistributed(Dense(1, activation='relu'))(u)\n",
    "x = Reshape((max_len,))(alpha)\n",
    "x = Activation('softmax')(x)\n",
    "x = Reshape((max_len, 1))(x)\n",
    "x = Multiply()([h, x])\n",
    "x = Lambda(lambda x: K.sum(x, axis=1))(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9785\n",
      " ROC-AUC - epoch: 1 - score: 0.982933 \n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98306, saving model to weights_base.best.h5\n",
      "143613/143613 [==============================] - 668s 5ms/step - loss: 0.0640 - acc: 0.9785 - val_loss: 0.0444 - val_acc: 0.9831\n",
      "Epoch 2/3\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9837\n",
      " ROC-AUC - epoch: 2 - score: 0.986470 \n",
      "\n",
      "Epoch 00002: val_acc improved from 0.98306 to 0.98388, saving model to weights_base.best.h5\n",
      "143613/143613 [==============================] - 662s 5ms/step - loss: 0.0424 - acc: 0.9837 - val_loss: 0.0422 - val_acc: 0.9839\n",
      "Epoch 3/3\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9852\n",
      " ROC-AUC - epoch: 3 - score: 0.987124 \n",
      "\n",
      "Epoch 00003: val_acc did not improve\n",
      "143613/143613 [==============================] - 659s 5ms/step - loss: 0.0373 - acc: 0.9852 - val_loss: 0.0429 - val_acc: 0.9839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6827146e10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=128\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=3, validation_data=(x_val, y_val), callbacks=callback_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
